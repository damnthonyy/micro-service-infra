import fs from "fs";
import OpenAI from "openai";
import dotenv from "dotenv";

dotenv.config();

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

async function generateLLMFeedback() {
  const reportPath = "spectral-report.json";
  if (!fs.existsSync(reportPath)) {
    console.error("âŒ Aucun fichier spectral-report.json trouvÃ© !");
    process.exit(1);
  }

  const report = JSON.parse(fs.readFileSync(reportPath, "utf8"));
  
  // Filtrer les erreurs par sÃ©vÃ©ritÃ© : ne garder que error (0) et warn (1)
  // Exclure info (2) et hint (3)
  const filteredReport = report.filter((issue) => {
    const severity = issue.severity ?? 0;
    return severity <= 1; // 0 = error, 1 = warn
  });

  console.log(`ğŸ“Š ${report.length} problÃ¨me(s) dÃ©tectÃ©(s), ${filteredReport.length} Ã  traiter (erreurs et avertissements uniquement)\n`);

  const explanations = [];

  for (const issue of filteredReport) {
    const prompt = `
    Voici une erreur Spectral dÃ©tectÃ©e lors de la validation d'une spec OpenAPI.
    Explique le problÃ¨me en termes simples et propose une correction :
    - RÃ¨gle : ${issue.code}
    - Message : ${issue.message}
    - Chemin : ${issue.path?.join(".")}
    `;

    try {
      const completion = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [{ role: "user", content: prompt }],
      });

      const explanation = completion.choices[0].message.content.trim();

      explanations.push({
        code: issue.code,
        message: issue.message,
        path: issue.path,
        explanation,
      });

      console.log(`ğŸ’¡ [${issue.code}] ${explanation}\n`);
    } catch (err) {
      console.error(`âš ï¸ Erreur lors de l'appel LLM : ${err.message}`);
    }
  }

  fs.writeFileSync("spectral-llm-report.json", JSON.stringify(explanations, null, 2));
  console.log("\nâœ… Rapport enrichi enregistrÃ© dans spectral-llm-report.json");
}

generateLLMFeedback();
